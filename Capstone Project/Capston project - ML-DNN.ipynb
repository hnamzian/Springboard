{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preparation Functions](#Preparation-Functions)\n",
    "* [Prepare data](#Prepare-data)\n",
    "* [data Normalization](#data-Normalization)\n",
    "* [Preprocessing](#Preprocessing)\n",
    "* [RandomForest Test](#RandomForest-Test)\n",
    "* [DNN model](#DNN-model)\n",
    "* [Cross Validation](#Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from scipy import stats\n",
    "import sklearn \n",
    "from sklearn.preprocessing import Imputer, StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD,RMSprop,Adam,Adadelta\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encode(X):\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(X[c].values)) \n",
    "            X[c] = lbl.transform(list(X[c].values))\n",
    "    return X\n",
    "\n",
    "def impute(X):\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    cols = X.columns\n",
    "    X_imp = pd.DataFrame(imp.fit_transform(X))\n",
    "    X_imp.columns = cols\n",
    "    return X_imp\n",
    "\n",
    "def Normalize(X):\n",
    "    cols = X.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = cols\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/Sber Bank/train.csv')\n",
    "train_cols = df_train.drop(['id', 'price_doc'], axis=1).columns\n",
    "\n",
    "df_test = pd.read_csv('./data/Sber Bank/test.csv')\n",
    "\n",
    "df_macro = pd.read_csv('./data/Sber Bank/macro.csv')\n",
    "macro_cols = df_macro.columns\n",
    "\n",
    "df_train_all = pd.merge(df_train, df_macro, on='timestamp')\n",
    "df_test_all = pd.merge(df_test, df_macro, on='timestamp')\n",
    "\n",
    "# keep 'timestamp' column in separate and drop it from dataset\n",
    "df_train_all.timestamp = pd.to_datetime(df_train.timestamp)\n",
    "df_train_timestamp = df_train.timestamp\n",
    "df_train_all = df_train_all.drop(['id', 'timestamp'], axis=1)\n",
    "\n",
    "df_test_all.timestamp = pd.to_datetime(df_test.timestamp)\n",
    "df_trest_timestamp = df_test.timestamp\n",
    "df_test_all = df_test_all.drop(['id', 'timestamp'], axis=1)\n",
    "\n",
    "# remove columns filled with NaN completely\n",
    "#bad_cols_test = df_test_all.columns[df_test_all.isnull().sum()==len(df_test_all)]\n",
    "#df_train_all = df_train_all.drop(bad_cols_test, axis=1)\n",
    "#df_test_all = df_test_all.drop(bad_cols_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_all = label_encode(df_train_all)\n",
    "df_train_all = impute(df_train_all)\n",
    "#df_train_all, scaler = Normalize(df_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train data:  (21329, 388)\n",
      "Length of y_train data:  (21329,)\n",
      "Length of X_test data:   (9142, 388)\n",
      "Length of y_test data:   (9142,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train_all.drop(['price_doc'], axis=1), \n",
    "                                                    df_train_all['price_doc'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Length of X_train data: ', X_train.shape) \n",
    "print('Length of y_train data: ', y_train.shape)\n",
    "print('Length of X_test data:  ', X_test.shape)\n",
    "print('Length of y_test data:  ', y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "X_train, scaler = Normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train data:  (21329, 388)\n",
      "Length of y_train data:  (21329,)\n",
      "Length of X_test data:   (9142, 388)\n",
      "Length of y_test data:   (9142,)\n"
     ]
    }
   ],
   "source": [
    "train = X_train.join(y_train)\n",
    "test = X_test.join(y_test)\n",
    "cols = test.columns\n",
    "train, scaler = Normalize(train)\n",
    "test = pd.DataFrame(scaler.transform(test))\n",
    "test.columns = cols\n",
    "\n",
    "y_train = train['price_doc']\n",
    "X_train = train.drop('price_doc', axis=1)\n",
    "y_test = test['price_doc']\n",
    "X_test = test.drop('price_doc', axis=1)\n",
    "\n",
    "print('Length of X_train data: ', X_train.shape) \n",
    "print('Length of y_train data: ', y_train.shape)\n",
    "print('Length of X_test data:  ', X_test.shape)\n",
    "print('Length of y_test data:  ', y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Root Mean Squared Log error\n",
    "def RMSLE(y, y_):\n",
    "    return np.sqrt(np.mean(np.log((y+1)/(y_+1))**2))\n",
    "\n",
    "def RMSE(y,y_):\n",
    "    return np.sqrt(np.mean((y-y_)**2))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_score(estimator, X, y, cv=5):\n",
    "    kf = KFold(cv)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = estimator.fit(X[train_index], y[train_index])\n",
    "        #y_ = model\n",
    "        \n",
    "def model_eval(estimator, X_train, y_train, X_test, y_test):\n",
    "    #train_cv_score = cross_val_score(xgr, X_train, y_train, cv=5)\n",
    "    y_train_pred = cross_val_predict(estimator, X_train, y_train, cv=5)\n",
    "    train_r2_score = r2_score(y_train, y_train_pred)\n",
    "    train_mse = np.sqrt(np.mean((y_train-y_train_pred)**2))\n",
    "    train_rmsle = RMSLE(y_train, y_train_pred)\n",
    "    \n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    y_test_pred = estimator.predict(X_test)\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_mse = np.sqrt(np.mean((y_test-y_test_pred)**2))\n",
    "    test_rmsle = RMSLE(y_test, y_test_pred)\n",
    "\n",
    "    #print 'train CV R2_Scores:     {}'.format(train_cv_score)\n",
    "    print('train RMSLE:            {}', train_rmsle)\n",
    "    print('train CV R2_Score:      {}', train_r2_score)\n",
    "    print('train mse:              {}', train_mse)\n",
    "    print('test rmsle:             {}', test_rmsle)\n",
    "    print('test R2_Score:          {}', test_r2_score)\n",
    "    print('test mse:               {}', test_mse)\n",
    "    \n",
    "    return y_train_pred, y_test_pred, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    lr=0.001\n",
    "    start=15\n",
    "    step=5\n",
    "    if epoch<start:\n",
    "        return lr\n",
    "    else:\n",
    "        lr=lr/np.power(2.0,(1+(epoch-start)/step))\n",
    "        return lr\n",
    "\n",
    "def nn_model(X):\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    #model_dnn.add(GaussianDropout(0.1))\n",
    "    model.add(Dense(2048, input_dim=X.shape[1], kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # layer 2\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1024, input_dim=2048))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # layer 3\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, input_dim=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # layer 4\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, input_dim=512))\n",
    "    \n",
    "    \n",
    "    optimizer=SGD(lr=0, momentum=0.5,nesterov=True,clipnorm=100)\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    \n",
    "    #model.fit(X, y, callbacks=[lrate], batch_size=128, epochs=64, verbose=1)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.6525    \n",
      "Epoch 2/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5607    \n",
      "Epoch 3/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5498    \n",
      "Epoch 4/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5466    \n",
      "Epoch 5/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5409    \n",
      "Epoch 6/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5368    \n",
      "Epoch 7/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5369    \n",
      "Epoch 8/8\n",
      "21329/21329 [==============================] - 22s - loss: 0.5333    \n",
      "train R2 -0.307528639194\n"
     ]
    }
   ],
   "source": [
    "dnn = nn_model(X_train)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "dnn.fit(X_train.values, y_train, callbacks=[lrate], batch_size=16, epochs=8, verbose=1)\n",
    "y_train_norm = dnn.predict(X_train.values)\n",
    "print('train R2', r2_score(y_train, y_train_norm))\n",
    "#print('train rmse', RMSE(y_train, y_train_norm))\n",
    "\n",
    "#y_test_norm = dnn.predict(X_test.values)\n",
    "\n",
    "#print ('test R2: ', r2_score(y_test, y_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = X_train.mean()\n",
    "s = X_train.std()\n",
    "X_train = (X_train - m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my = y_train.mean()\n",
    "sy = y_train.std()\n",
    "y_train = (y_train-my)/sy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
