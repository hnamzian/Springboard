{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preparation Functions](#Preparation-Functions)\n",
    "* [Prepare data](#Prepare-data)\n",
    "* [data Normalization](#data-Normalization)\n",
    "* [Preprocessing](#Preprocessing)\n",
    "* [RandomForest Test](#RandomForest-Test)\n",
    "* [DNN model](#DNN-model)\n",
    "* [Cross Validation](#Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "from scipy import stats\n",
    "import sklearn \n",
    "from sklearn.preprocessing import Imputer, StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD,RMSprop,Adam,Adadelta\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encode(X):\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(X[c].values)) \n",
    "            X[c] = lbl.transform(list(X[c].values))\n",
    "    return X\n",
    "\n",
    "def impute(X):\n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    cols = X.columns\n",
    "    X_imp = pd.DataFrame(imp.fit_transform(X))\n",
    "    X_imp.columns = cols\n",
    "    return X_imp\n",
    "\n",
    "def Normalize(X):\n",
    "    cols = X.columns\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = cols\n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/Sber Bank/train.csv')\n",
    "train_cols = df_train.drop(['id', 'price_doc'], axis=1).columns\n",
    "\n",
    "df_test = pd.read_csv('./data/Sber Bank/test.csv')\n",
    "\n",
    "df_macro = pd.read_csv('./data/Sber Bank/macro.csv')\n",
    "macro_cols = df_macro.columns\n",
    "\n",
    "df_train_all = pd.merge(df_train, df_macro, on='timestamp')\n",
    "df_test_all = pd.merge(df_test, df_macro, on='timestamp')\n",
    "\n",
    "# keep 'timestamp' column in separate and drop it from dataset\n",
    "df_train_all.timestamp = pd.to_datetime(df_train.timestamp)\n",
    "df_train_timestamp = df_train.timestamp\n",
    "df_train_all = df_train_all.drop(['id', 'timestamp'], axis=1)\n",
    "\n",
    "df_test_all.timestamp = pd.to_datetime(df_test.timestamp)\n",
    "df_trest_timestamp = df_test.timestamp\n",
    "df_test_all = df_test_all.drop(['id', 'timestamp'], axis=1)\n",
    "\n",
    "# remove columns filled with NaN completely\n",
    "#bad_cols_test = df_test_all.columns[df_test_all.isnull().sum()==len(df_test_all)]\n",
    "#df_train_all = df_train_all.drop(bad_cols_test, axis=1)\n",
    "#df_test_all = df_test_all.drop(bad_cols_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_all = label_encode(df_train_all)\n",
    "df_train_all = impute(df_train_all)\n",
    "#df_train_all, scaler = Normalize(df_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train data:  (21329, 388)\n",
      "Length of y_train data:  (21329,)\n",
      "Length of X_test data:   (9142, 388)\n",
      "Length of y_test data:   (9142,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train_all.drop(['price_doc'], axis=1), \n",
    "                                                    df_train_all['price_doc'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Length of X_train data: ', X_train.shape) \n",
    "print('Length of y_train data: ', y_train.shape)\n",
    "print('Length of X_test data:  ', X_test.shape)\n",
    "print('Length of y_test data:  ', y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = X_train.columns\n",
    "X_train, scaler = Normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train data:  (21329, 388)\n",
      "Length of y_train data:  (21329,)\n",
      "Length of X_test data:   (9142, 388)\n",
      "Length of y_test data:   (9142,)\n"
     ]
    }
   ],
   "source": [
    "#train = X_train.join(y_train)\n",
    "#test = X_test.join(y_test)\n",
    "cols = test.columns\n",
    "train, scaler = Normalize(train)\n",
    "test = pd.DataFrame(scaler.transform(test))\n",
    "test.columns = cols\n",
    "\n",
    "y_train = train['price_doc']\n",
    "X_train = train.drop('price_doc', axis=1)\n",
    "y_test = test['price_doc']\n",
    "X_test = test.drop('price_doc', axis=1)\n",
    "\n",
    "print('Length of X_train data: ', X_train.shape) \n",
    "print('Length of y_train data: ', y_train.shape)\n",
    "print('Length of X_test data:  ', X_test.shape)\n",
    "print('Length of y_test data:  ', y_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Root Mean Squared Log error\n",
    "def RMSLE(y, y_):\n",
    "    return np.sqrt(np.mean(np.log((y+1)/(y_+1))**2))\n",
    "\n",
    "def RMSE(y,y_):\n",
    "    return np.sqrt(np.mean((y-y_)**2))\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_score(estimator, X, y, cv=5):\n",
    "    kf = KFold(cv)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        model = estimator.fit(X[train_index], y[train_index])\n",
    "        #y_ = model\n",
    "        \n",
    "def model_eval(estimator, X_train, y_train, X_test, y_test):\n",
    "    #train_cv_score = cross_val_score(xgr, X_train, y_train, cv=5)\n",
    "    y_train_pred = cross_val_predict(estimator, X_train, y_train, cv=5)\n",
    "    train_r2_score = r2_score(y_train, y_train_pred)\n",
    "    train_mse = np.sqrt(np.mean((y_train-y_train_pred)**2))\n",
    "    train_rmsle = RMSLE(y_train, y_train_pred)\n",
    "    \n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    y_test_pred = estimator.predict(X_test)\n",
    "    test_r2_score = r2_score(y_test, y_test_pred)\n",
    "    test_mse = np.sqrt(np.mean((y_test-y_test_pred)**2))\n",
    "    test_rmsle = RMSLE(y_test, y_test_pred)\n",
    "\n",
    "    #print 'train CV R2_Scores:     {}'.format(train_cv_score)\n",
    "    print('train RMSLE:            {}', train_rmsle)\n",
    "    print('train CV R2_Score:      {}', train_r2_score)\n",
    "    print('train mse:              {}', train_mse)\n",
    "    print('test rmsle:             {}', test_rmsle)\n",
    "    print('test R2_Score:          {}', test_r2_score)\n",
    "    print('test mse:               {}', test_mse)\n",
    "    \n",
    "    return y_train_pred, y_test_pred, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSLE:            {} 0.480225508308\n",
      "train CV R2_Score:      {} 0.631027301183\n",
      "train mse:              {} 2940369.97915\n",
      "test rmsle:             {} 0.477649639034\n",
      "test R2_Score:          {} 0.663161074746\n",
      "test mse:               {} 2690123.42147\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "_ = model_eval(rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49664784,  0.13548171,  0.21327295, ..., -1.26925155,\n",
       "       -0.11406502, -0.73214766])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_ = np.mean(y_train)\n",
    "std_ = y_train.std()\n",
    "y_ = y_train.values-mean_\n",
    "y_ = y_/std_\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_norm = y_train_norm.reshape(len(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.36312768414760255"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (np.sum((y_train_norm-y_)**2))/(np.sum(y_**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    lr=0.1\n",
    "    start=127\n",
    "    step=40\n",
    "    if epoch<start:\n",
    "        return lr\n",
    "    else:\n",
    "        lr=lr/np.power(2.0,(1+(epoch-start)/step))\n",
    "        return lr\n",
    "\n",
    "def nn_model(X, y):\n",
    "    \n",
    "    #lrate = LearningRateScheduler(step_decay)\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # layer 1\n",
    "    #model_dnn.add(GaussianDropout(0.1))\n",
    "    model.add(Dense(2048, input_dim=X.shape[1], kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # layer 2\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024, input_dim=2048))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # layer 3\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, input_dim=1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # layer 4\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, input_dim=256))\n",
    "    \n",
    "    \n",
    "    optimizer=SGD(lr=0, momentum=0.5,nesterov=True,clipnorm=100)\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    \n",
    "    #model.fit(X, y, callbacks=[lrate], batch_size=128, epochs=64, verbose=1)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.7922     \n",
      "Epoch 2/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.5109     \n",
      "Epoch 3/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4923     \n",
      "Epoch 4/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4809     \n",
      "Epoch 5/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4727     \n",
      "Epoch 6/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4723     \n",
      "Epoch 7/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4786     \n",
      "Epoch 8/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4788     \n",
      "Epoch 9/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4852     \n",
      "Epoch 10/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4808     \n",
      "Epoch 11/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4773     \n",
      "Epoch 12/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4772     \n",
      "Epoch 13/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4803     \n",
      "Epoch 14/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4771     \n",
      "Epoch 15/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4734     \n",
      "Epoch 16/16\n",
      "21329/21329 [==============================] - 3s - loss: 0.4705     \n",
      "train R2 -0.375757765988\n",
      "train rmse 1.51526226188\n"
     ]
    }
   ],
   "source": [
    "dnn = nn_model(X_train.values, y_)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "dnn.fit(X_train.values, y_, callbacks=[lrate], batch_size=128, epochs=16, verbose=1)\n",
    "y_train_norm = dnn.predict(X_train.values)\n",
    "print('train R2', r2_score(y_, y_train_norm))\n",
    "print('train rmse', RMSE(y_, y_train_norm))\n",
    "\n",
    "#y_test_norm = dnn.predict(X_test.values)\n",
    "\n",
    "#print ('test R2: ', r2_score(y_test, y_test_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_score(X, y, cv=5):\n",
    "    y_ = np.zeros([len(y),1])\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        dnn = nn_model()\n",
    "        dnn.fit(X, y, callbacks=[lrate], batch_size=128, epochs=64, verbose=1)\n",
    "        y_[test_index] = dnn.predict(X[test_index])\n",
    "    r2 = r2_score(y, y_)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "17063/17063 [==============================] - 2s - loss: nan     \n",
      "Epoch 2/32\n",
      "17063/17063 [==============================] - 2s - loss: nan     \n",
      "Epoch 3/32\n",
      "17063/17063 [==============================] - 2s - loss: nan     \n",
      "Epoch 4/32\n",
      "17063/17063 [==============================] - 2s - loss: nan     \n",
      "Epoch 5/32\n",
      " 5760/17063 [=========>....................] - ETA: 1s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-339-32e074b69d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlrate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0my_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hossein\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = X_train_norm.values\n",
    "Y = y_train.copy()\n",
    "kf = KFold(5)\n",
    "y_ = np.zeros([len(y),1])\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x = X[train_index]\n",
    "    y = Y[train_index]\n",
    "    dnn = nn_model()\n",
    "    dnn.fit(x, y, callbacks=[lrate], batch_size=128, epochs=32, verbose=1)\n",
    "    y_[test_index] = dnn.predict(X[test_index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
